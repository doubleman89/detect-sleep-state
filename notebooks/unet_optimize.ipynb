{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config \n",
    "import configs.config\n",
    "importlib.reload(configs.config)\n",
    "from configs.config import CFG\n",
    "from utils import config\n",
    "config = config.Config.from_json(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4405, 150, 5)\n",
      "(45, 10, 5)\n"
     ]
    }
   ],
   "source": [
    "# create Train Series and Dataset \n",
    "import dataloader.dataloader\n",
    "importlib.reload(dataloader.dataloader)\n",
    "from dataloader.dataloader import Series,Dataset\n",
    "try: \n",
    "    del ds\n",
    "except:\n",
    "    pass\n",
    "from dataloader.dataloader import Test_Series, Train_Series\n",
    "try: \n",
    "    del test_series , train_series\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_series = Train_Series(config.data,config.paths)\n",
    "train_series.createSeries()\n",
    "test_series = Test_Series(config.data,config.paths)\n",
    "test_series.createSeries()\n",
    "ds = Dataset(train_series,test_series)\n",
    "ds.split_dataset()\n",
    "\n",
    "X_train = ds.X_train\n",
    "y_train = ds.y_train\n",
    "X_test = ds.X_test\n",
    "y_test = ds.y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datset to Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Include the epoch in the file name (uses `str.format`)\n",
    "# ds_path = \"./model/saved_ds/ds.obj\"\n",
    "\n",
    "# with  open(ds_path,'wb') as file:\n",
    "#     exampleObj = pickle.dump(ds,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_Pickler.__init__() got an unexpected keyword argument 'ignore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ds_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/saved_ds/ds.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ds_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 4\u001b[0m     my_pickle \u001b[38;5;241m=\u001b[39m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\dill\\_dill.py:250\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[0;32m    248\u001b[0m _kwds \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    249\u001b[0m _kwds\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(byref\u001b[38;5;241m=\u001b[39mbyref, fmode\u001b[38;5;241m=\u001b[39mfmode, recurse\u001b[38;5;241m=\u001b[39mrecurse))\n\u001b[1;32m--> 250\u001b[0m Pickler(file, protocol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwds)\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\dill\\_dill.py:355\u001b[0m, in \u001b[0;36mPickler.__init__\u001b[1;34m(self, file, *args, **kwds)\u001b[0m\n\u001b[0;32m    353\u001b[0m _fmode \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    354\u001b[0m _recurse \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecurse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 355\u001b[0m StockPickler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main \u001b[38;5;241m=\u001b[39m _main_module\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: _Pickler.__init__() got an unexpected keyword argument 'ignore'"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "ds_path = \"./model/saved_ds/ds.pkl\"\n",
    "with open(ds_path,'wb') as file:\n",
    "    my_pickle = dill.dump(ds,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '__module__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '__module__'"
     ]
    }
   ],
   "source": [
    "ds.__module__.__module__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test Series to Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "test_path = \"./model/saved_ds/test_series.obj\"\n",
    "\n",
    "with  open(test_path,'wb') as file:\n",
    "    exampleObj = pickle.dump(ds,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call model and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 160, 2)]     0           []                               \n",
      "                                                                                                  \n",
      " downsampling_block (Downsampli  ((None, 80, 8),     256         ['input_1[0][0]']                \n",
      " ngBlock)                        (None, 160, 8))                                                  \n",
      "                                                                                                  \n",
      " downsampling_block_1 (Downsamp  ((None, 40, 16),    1184        ['downsampling_block[0][0]']     \n",
      " lingBlock)                      (None, 80, 16))                                                  \n",
      "                                                                                                  \n",
      " downsampling_block_2 (Downsamp  ((None, 20, 32),    4672        ['downsampling_block_1[0][0]']   \n",
      " lingBlock)                      (None, 40, 32))                                                  \n",
      "                                                                                                  \n",
      " downsampling_block_3 (Downsamp  ((None, 10, 64),    18560       ['downsampling_block_2[0][0]']   \n",
      " lingBlock)                      (None, 20, 64))                                                  \n",
      "                                                                                                  \n",
      " downsampling_block_4 (Downsamp  ((None, 10, 128),   73984       ['downsampling_block_3[0][0]']   \n",
      " lingBlock)                      (None, 10, 128))                                                 \n",
      "                                                                                                  \n",
      " upsampling_block (UpsamplingBl  (None, 20, 64)      61632       ['downsampling_block_4[0][0]',   \n",
      " ock)                                                             'downsampling_block_3[0][1]']   \n",
      "                                                                                                  \n",
      " upsampling_block_1 (Upsampling  (None, 40, 32)      15456       ['upsampling_block[0][0]',       \n",
      " Block)                                                           'downsampling_block_2[0][1]']   \n",
      "                                                                                                  \n",
      " upsampling_block_2 (Upsampling  (None, 80, 16)      3888        ['upsampling_block_1[0][0]',     \n",
      " Block)                                                           'downsampling_block_1[0][1]']   \n",
      "                                                                                                  \n",
      " upsampling_block_3 (Upsampling  (None, 160, 8)      984         ['upsampling_block_2[0][0]',     \n",
      " Block)                                                           'downsampling_block[0][1]']     \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 160, 4)       100         ['upsampling_block_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 160, 3)       15          ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 180,731\n",
      "Trainable params: 180,731\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from model.model1D import Unet,Unet_Model\n",
    "import model.model1D\n",
    "importlib.reload(model.model1D)\n",
    "try: \n",
    "    del unet\n",
    "except:\n",
    "    pass\n",
    "\n",
    "unet = Unet(n_classes=3,n_filters=8)\n",
    "unet.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "unet.model((160,2)).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and save weights and model to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1006, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<keras.losses.SparseCategoricalCrossentropy object at 0x000001CEFBCB1EA0>, and therefore expects target data to be provided in `fit()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\Kaggle\\Detect-sleep-state\\unet_test2.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m history \u001b[39m=\u001b[39m unet\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Include the epoch in the file name (uses `str.format`)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel/weights/cp-\u001b[39m\u001b[39m{epoch:04d}\u001b[39;00m\u001b[39m.ckpt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\PAWEL~1.DUB\\AppData\\Local\\Temp\\__autograph_generated_file8x680pi5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"d:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1006, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=<keras.losses.SparseCategoricalCrossentropy object at 0x000001CEFBCB1EA0>, and therefore expects target data to be provided in `fit()`.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#train model\n",
    "history = unet.fit(X_train,y_train,epochs = 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"model/weights/cp-{epoch:04d}.ckpt\"\n",
    "# save weights \n",
    "unet.save_weights(checkpoint_path.format(epoch=100))\n",
    "#save model\n",
    "models_path = \"model/saved_models/unet_{version}_{now}\"\n",
    "unet.save(models_path.format(version = \"v1\", now = str(datetime.now().date())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2be5c0181f0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"model/weights/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# Loads the weights\n",
    "unet.load_weights(checkpoint_path.format(epoch =100))\n",
    "\n",
    "# Re-evaluate the model\n",
    "# loss, acc = unet.evaluate(test_images, test_labels, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Train Series and Dataset \n",
    "import dataloader.dataloader\n",
    "importlib.reload(dataloader.dataloader)\n",
    "from dataloader.dataloader import Test_Series\n",
    "try: \n",
    "    del test_series\n",
    "except:\n",
    "    pass\n",
    "\n",
    "test_series = Test_Series(config.data,config.paths)\n",
    "test_series.createSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_events(y_pred :np.array,serie):\n",
    "    \"\"\"create df_events \n",
    "    input:\n",
    "    y_pred - score  (predicited vaues )\"\"\"\n",
    "    def detectChange(last_val,current_val):\n",
    "        if last_val == -1 or current_val == -1:\n",
    "            return False\n",
    "\n",
    "        return last_val !=current_val\n",
    "\n",
    "    # create empty dataframe\n",
    "    df_events = pd.DataFrame(columns=[\"series_id\",\"step\",\"event\",\"score\"])\n",
    "    # unpad y_pred\n",
    "    if serie.slice_pads is None:\n",
    "        y_pred_unpadded = y_pred\n",
    "    else:\n",
    "        y_pred_unpadded = serie._unpad(y_pred,serie.slice_pads)\n",
    "    # create step seg list\n",
    "    #         - events - segmentation mask\n",
    "    #         -  score   - predicited vaues for chosen event\n",
    "    events = np.argmax(y_pred_unpadded,axis = -1,keepdims=True)\n",
    "    score = np.max(y_pred_unpadded,axis=-1,keepdims=True)      \n",
    "    for slice_num in range(events.shape[0]):\n",
    "        for i in range(events.shape[-2]):\n",
    "            event_val = events[slice_num,i][0]\n",
    "            event_score = score[slice_num,i][0]\n",
    "            if i == 0:\n",
    "                # do not detect anything during first step\n",
    "                continue\n",
    "            elif not detectChange(events[slice_num,i-1],event_val):\n",
    "                continue\n",
    "\n",
    "            df_events.loc[len(df_events.index)] = [serie.serie_id,i,event_val,event_score]\n",
    "    \n",
    "    # decode events \n",
    "    print(df_events)\n",
    "    df_events = serie.decode_events(df_events)\n",
    "    # save as serie events \n",
    "    serie_events = df_events\n",
    "\n",
    "    return serie_events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      series_id  step  event      score\n",
      "0  038441c925bb     5      2  29.746996\n",
      "1  038441c925bb    17      1  46.573971\n",
      "2  038441c925bb    22      2  33.335999\n",
      "3  038441c925bb    25      1  47.931240\n",
      "      series_id  step   event      score\n",
      "0  038441c925bb     5   onset  29.746996\n",
      "1  038441c925bb    17  wakeup  46.573971\n",
      "2  038441c925bb    22   onset  33.335999\n",
      "3  038441c925bb    25  wakeup  47.931240\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Empty DataFrame\n",
      "Columns: [series_id, step, event, score]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [series_id, step, event, score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def predict(model,test_serie):\n",
    "    x = test_serie.slices\n",
    "    y_pred = model.predict(x)\n",
    "    serie_events = create_events(y_pred,test_serie)   \n",
    "    return serie_events\n",
    "\n",
    "test_series_pred = {}\n",
    "for serie_id, test_serie in test_series.series.items():\n",
    "    serie_events = predict(unet,test_serie)\n",
    "    print(serie_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\Kaggle\\Detect-sleep-state\\unet_test2.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test2.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real,pred):\n",
    "    comp = real == pred\n",
    "    return np.sum(comp)/(comp.shape[0]*comp.shape[1])\n",
    "\n",
    "\n",
    "y_train_pred = unet.predict(X_train)\n",
    "print(\"train accuracy : \" + str(accuracy(y_train,y_train_pred)))\n",
    "\n",
    "\n",
    "y_test_pred = unet.predict(X_test)\n",
    "print(\"test accuracy : \" +  str(accuracy(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(51, 8640, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\Kaggle\\Detect-sleep-state\\unet_test.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# os.chdir(r'kaggle/working')'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame(test_mask)\u001b[39m.\u001b[39mto_csv(filename)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DEV/Projekty/PythonProjects/Kaggle/Detect-sleep-state/unet_test.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m FileLink(filename)\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\pandas\\core\\frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    747\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    748\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    749\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    755\u001b[0m             copy\u001b[39m=\u001b[39m_copy,\n\u001b[0;32m    756\u001b[0m         )\n\u001b[0;32m    757\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    759\u001b[0m             data,\n\u001b[0;32m    760\u001b[0m             index,\n\u001b[0;32m    761\u001b[0m             columns,\n\u001b[0;32m    762\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    763\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    764\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    765\u001b[0m         )\n\u001b[0;32m    767\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:315\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    309\u001b[0m     _copy \u001b[39m=\u001b[39m (\n\u001b[0;32m    310\u001b[0m         copy_on_sanitize\n\u001b[0;32m    311\u001b[0m         \u001b[39mif\u001b[39;00m (dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m astype_is_view(values\u001b[39m.\u001b[39mdtype, dtype))\n\u001b[0;32m    312\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    313\u001b[0m     )\n\u001b[0;32m    314\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(values, copy\u001b[39m=\u001b[39m_copy)\n\u001b[1;32m--> 315\u001b[0m     values \u001b[39m=\u001b[39m _ensure_2d(values)\n\u001b[0;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarraylike(values, copy\u001b[39m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[1;32md:\\DEV\\Projekty\\PythonProjects\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:570\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    568\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m    569\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 570\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(51, 8640, 3)"
     ]
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "import pandas as pd\n",
    "\n",
    "# import os\n",
    "# os.chdir(r'kaggle/working')'\n",
    "filename = 'train_' +'.csv'\n",
    "pd.DataFrame(test_mask).to_csv(filename)\n",
    "\n",
    "FileLink(filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_projects",
   "language": "python",
   "name": "venv_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
